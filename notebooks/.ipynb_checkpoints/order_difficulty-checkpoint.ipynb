{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "import inclearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 1\n"
     ]
    }
   ],
   "source": [
    "inclearn.train._set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.cifar.CIFAR100(\"data\", download=True, train=True, transform=train_transforms)\n",
    "test_dataset= datasets.cifar.CIFAR100(\"data\", download=True, train=False, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, num_workers=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(model, loader):\n",
    "    predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        all_targets.append(targets.numpy())\n",
    "        \n",
    "        logits = model(inputs)\n",
    "        predictions.append(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    total_acc = (predictions == all_targets).sum() / len(all_targets)\n",
    "    \n",
    "    return all_targets, predictions, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "\n",
    "    #BasicBlock and BottleNeck block \n",
    "    #have different output size\n",
    "    #we use class attribute expansion\n",
    "    #to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        #we use a different inputsize than the original paper\n",
    "        #so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the \n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may \n",
    "        contain more than one residual block \n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "        \n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block \n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output \n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def resnet34():\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = inclearn.lib.network.BasicNet(\"rebuffi\", device=device, use_bias=True)\n",
    "#model.add_classes(100)\n",
    "\n",
    "#model = resnet34().to(device)\n",
    "model = torchvision.models.resnet34(num_classes=100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "scheduling = [60, 120, 160]\n",
    "gamma = 0.2\n",
    "warmup_epochs = 1\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, nesterov=True, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, scheduling, gamma=gamma)\n",
    "warmup = WarmUpLR(optimizer, len(train_loader) * warmup_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200, train_loss: 4.666, train_acc: 0.01, test_acc: 0.01\n",
      "Best acc! Saving model\n",
      "Epoch 1/200, train_loss: 4.086, train_acc: 0.11, test_acc: 0.11\n",
      "Best acc! Saving model\n",
      "Epoch 2/200, train_loss: 3.488, train_acc: 0.18, test_acc: 0.18\n",
      "Best acc! Saving model\n",
      "Epoch 3/200, train_loss: 3.083, train_acc: 0.26, test_acc: 0.25\n",
      "Best acc! Saving model\n",
      "Epoch 4/200, train_loss: 2.746, train_acc: 0.33, test_acc: 0.34\n",
      "Best acc! Saving model\n",
      "Epoch 5/200, train_loss: 2.463, train_acc: 0.36, test_acc: 0.36\n",
      "Best acc! Saving model\n",
      "Epoch 6/200, train_loss: 2.23, train_acc: 0.4, test_acc: 0.41\n",
      "Best acc! Saving model\n",
      "Epoch 7/200, train_loss: 2.044, train_acc: 0.45, test_acc: 0.45\n",
      "Best acc! Saving model\n",
      "Epoch 8/200, train_loss: 1.912, train_acc: 0.46, test_acc: 0.44\n",
      "Epoch 9/200, train_loss: 1.803, train_acc: 0.47, test_acc: 0.46\n",
      "Best acc! Saving model\n",
      "Epoch 10/200, train_loss: 1.721, train_acc: 0.48, test_acc: 0.47\n",
      "Best acc! Saving model\n",
      "Epoch 11/200, train_loss: 1.653, train_acc: 0.51, test_acc: 0.49\n",
      "Best acc! Saving model\n",
      "Epoch 12/200, train_loss: 1.595, train_acc: 0.51, test_acc: 0.49\n",
      "Epoch 13/200, train_loss: 1.552, train_acc: 0.55, test_acc: 0.54\n",
      "Best acc! Saving model\n",
      "Epoch 14/200, train_loss: 1.512, train_acc: 0.54, test_acc: 0.52\n",
      "Epoch 15/200, train_loss: 1.478, train_acc: 0.57, test_acc: 0.54\n",
      "Best acc! Saving model\n",
      "Epoch 16/200, train_loss: 1.449, train_acc: 0.5, test_acc: 0.49\n",
      "Epoch 17/200, train_loss: 1.411, train_acc: 0.55, test_acc: 0.54\n",
      "Epoch 18/200, train_loss: 1.399, train_acc: 0.61, test_acc: 0.57\n",
      "Best acc! Saving model\n",
      "Epoch 19/200, train_loss: 1.373, train_acc: 0.59, test_acc: 0.55\n",
      "Epoch 20/200, train_loss: 1.355, train_acc: 0.58, test_acc: 0.55\n",
      "Epoch 21/200, train_loss: 1.332, train_acc: 0.54, test_acc: 0.53\n",
      "Epoch 22/200, train_loss: 1.323, train_acc: 0.6, test_acc: 0.57\n",
      "Best acc! Saving model\n",
      "Epoch 23/200, train_loss: 1.308, train_acc: 0.55, test_acc: 0.53\n",
      "Epoch 24/200, train_loss: 1.294, train_acc: 0.61, test_acc: 0.57\n",
      "Best acc! Saving model\n",
      "Epoch 25/200, train_loss: 1.286, train_acc: 0.6, test_acc: 0.57\n",
      "Epoch 26/200, train_loss: 1.268, train_acc: 0.62, test_acc: 0.57\n",
      "Epoch 27/200, train_loss: 1.254, train_acc: 0.58, test_acc: 0.54\n",
      "Epoch 28/200, train_loss: 1.259, train_acc: 0.62, test_acc: 0.58\n",
      "Best acc! Saving model\n",
      "Epoch 29/200, train_loss: 1.24, train_acc: 0.61, test_acc: 0.57\n",
      "Epoch 31/200, train_loss: 1.236, train_acc: 0.62, test_acc: 0.57\n",
      "Epoch 32/200, train_loss: 1.221, train_acc: 0.61, test_acc: 0.58\n",
      "Best acc! Saving model\n",
      "Epoch 33/200, train_loss: 1.217, train_acc: 0.57, test_acc: 0.55\n",
      "Epoch 34/200, train_loss: 1.206, train_acc: 0.6, test_acc: 0.56\n",
      "Epoch 35/200, train_loss: 1.199, train_acc: 0.62, test_acc: 0.58\n",
      "Best acc! Saving model\n",
      "Epoch 36/200, train_loss: 1.192, train_acc: 0.6, test_acc: 0.56\n",
      "Epoch 37/200, train_loss: 1.199, train_acc: 0.63, test_acc: 0.57\n",
      "Epoch 38/200, train_loss: 1.179, train_acc: 0.63, test_acc: 0.58\n",
      "Epoch 39/200, train_loss: 1.187, train_acc: 0.63, test_acc: 0.59\n",
      "Best acc! Saving model\n",
      "Epoch 40/200, train_loss: 1.171, train_acc: 0.62, test_acc: 0.57\n",
      "Epoch 41/200, train_loss: 1.178, train_acc: 0.63, test_acc: 0.59\n",
      "Epoch 42/200, train_loss: 1.169, train_acc: 0.61, test_acc: 0.58\n",
      "Epoch 43/200, train_loss: 1.165, train_acc: 0.62, test_acc: 0.59\n",
      "Epoch 44/200, train_loss: 1.157, train_acc: 0.64, test_acc: 0.59\n",
      "Epoch 45/200, train_loss: 1.156, train_acc: 0.61, test_acc: 0.56\n",
      "Epoch 46/200, train_loss: 1.152, train_acc: 0.64, test_acc: 0.59\n",
      "Best acc! Saving model\n",
      "Epoch 47/200, train_loss: 1.148, train_acc: 0.63, test_acc: 0.57\n",
      "Epoch 48/200, train_loss: 1.142, train_acc: 0.64, test_acc: 0.6\n",
      "Best acc! Saving model\n",
      "Epoch 49/200, train_loss: 1.145, train_acc: 0.62, test_acc: 0.58\n",
      "Epoch 50/200, train_loss: 1.14, train_acc: 0.65, test_acc: 0.6\n",
      "Epoch 51/200, train_loss: 1.137, train_acc: 0.64, test_acc: 0.59\n",
      "Epoch 52/200, train_loss: 1.128, train_acc: 0.63, test_acc: 0.59\n",
      "Epoch 53/200, train_loss: 1.129, train_acc: 0.63, test_acc: 0.59\n",
      "Epoch 54/200, train_loss: 1.131, train_acc: 0.64, test_acc: 0.58\n",
      "Epoch 55/200, train_loss: 1.13, train_acc: 0.63, test_acc: 0.58\n",
      "Epoch 56/200, train_loss: 1.125, train_acc: 0.63, test_acc: 0.59\n",
      "Epoch 57/200, train_loss: 1.121, train_acc: 0.62, test_acc: 0.58\n",
      "Epoch 58/200, train_loss: 1.121, train_acc: 0.65, test_acc: 0.59\n",
      "Epoch 59/200, train_loss: 1.118, train_acc: 0.66, test_acc: 0.6\n",
      "Epoch 60/200, train_loss: 0.654, train_acc: 0.86, test_acc: 0.73\n",
      "Best acc! Saving model\n",
      "Epoch 61/200, train_loss: 0.499, train_acc: 0.88, test_acc: 0.73\n",
      "Best acc! Saving model\n",
      "Epoch 62/200, train_loss: 0.439, train_acc: 0.89, test_acc: 0.73\n",
      "Epoch 63/200, train_loss: 0.398, train_acc: 0.9, test_acc: 0.73\n",
      "Epoch 64/200, train_loss: 0.363, train_acc: 0.91, test_acc: 0.73\n",
      "Epoch 65/200, train_loss: 0.339, train_acc: 0.91, test_acc: 0.73\n",
      "Epoch 66/200, train_loss: 0.322, train_acc: 0.91, test_acc: 0.72\n",
      "Epoch 67/200, train_loss: 0.324, train_acc: 0.9, test_acc: 0.71\n",
      "Epoch 68/200, train_loss: 0.315, train_acc: 0.92, test_acc: 0.72\n",
      "Epoch 69/200, train_loss: 0.313, train_acc: 0.91, test_acc: 0.71\n",
      "Epoch 70/200, train_loss: 0.319, train_acc: 0.92, test_acc: 0.71\n",
      "Epoch 71/200, train_loss: 0.308, train_acc: 0.92, test_acc: 0.71\n",
      "Epoch 72/200, train_loss: 0.317, train_acc: 0.9, test_acc: 0.71\n",
      "Epoch 73/200, train_loss: 0.307, train_acc: 0.9, test_acc: 0.7\n",
      "Epoch 74/200, train_loss: 0.319, train_acc: 0.91, test_acc: 0.7\n",
      "Epoch 75/200, train_loss: 0.312, train_acc: 0.9, test_acc: 0.7\n",
      "Epoch 76/200, train_loss: 0.322, train_acc: 0.9, test_acc: 0.7\n",
      "Epoch 77/200, train_loss: 0.327, train_acc: 0.89, test_acc: 0.69\n",
      "Epoch 78/200, train_loss: 0.313, train_acc: 0.9, test_acc: 0.7\n",
      "Epoch 79/200, train_loss: 0.317, train_acc: 0.91, test_acc: 0.69\n",
      "Epoch 80/200, train_loss: 0.313, train_acc: 0.9, test_acc: 0.69\n",
      "Epoch 81/200, train_loss: 0.3, train_acc: 0.89, test_acc: 0.67\n",
      "Epoch 82/200, train_loss: 0.312, train_acc: 0.9, test_acc: 0.7\n",
      "Epoch 83/200, train_loss: 0.303, train_acc: 0.9, test_acc: 0.69\n",
      "Epoch 84/200, train_loss: 0.3, train_acc: 0.89, test_acc: 0.69\n",
      "Epoch 85/200, train_loss: 0.294, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 86/200, train_loss: 0.287, train_acc: 0.91, test_acc: 0.69\n",
      "Epoch 87/200, train_loss: 0.296, train_acc: 0.9, test_acc: 0.68\n",
      "Epoch 88/200, train_loss: 0.286, train_acc: 0.89, test_acc: 0.69\n",
      "Epoch 89/200, train_loss: 0.293, train_acc: 0.9, test_acc: 0.69\n",
      "Epoch 90/200, train_loss: 0.287, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 91/200, train_loss: 0.29, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 92/200, train_loss: 0.287, train_acc: 0.91, test_acc: 0.69\n",
      "Epoch 93/200, train_loss: 0.275, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 94/200, train_loss: 0.277, train_acc: 0.9, test_acc: 0.67\n",
      "Epoch 95/200, train_loss: 0.276, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 96/200, train_loss: 0.262, train_acc: 0.92, test_acc: 0.68\n",
      "Epoch 97/200, train_loss: 0.267, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 98/200, train_loss: 0.261, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 99/200, train_loss: 0.27, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 100/200, train_loss: 0.263, train_acc: 0.9, test_acc: 0.68\n",
      "Epoch 101/200, train_loss: 0.261, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 102/200, train_loss: 0.264, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 103/200, train_loss: 0.26, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 104/200, train_loss: 0.267, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 105/200, train_loss: 0.253, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 106/200, train_loss: 0.254, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 107/200, train_loss: 0.262, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 108/200, train_loss: 0.261, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 109/200, train_loss: 0.254, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 110/200, train_loss: 0.253, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 111/200, train_loss: 0.257, train_acc: 0.91, test_acc: 0.67\n",
      "Epoch 112/200, train_loss: 0.243, train_acc: 0.92, test_acc: 0.69\n",
      "Epoch 113/200, train_loss: 0.253, train_acc: 0.92, test_acc: 0.7\n",
      "Epoch 114/200, train_loss: 0.255, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 115/200, train_loss: 0.256, train_acc: 0.91, test_acc: 0.68\n",
      "Epoch 116/200, train_loss: 0.242, train_acc: 0.89, test_acc: 0.66\n",
      "Epoch 117/200, train_loss: 0.242, train_acc: 0.93, test_acc: 0.69\n",
      "Epoch 118/200, train_loss: 0.247, train_acc: 0.92, test_acc: 0.68\n",
      "Epoch 119/200, train_loss: 0.24, train_acc: 0.93, test_acc: 0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200, train_loss: 0.094, train_acc: 0.99, test_acc: 0.74\n",
      "Best acc! Saving model\n",
      "Epoch 121/200, train_loss: 0.05, train_acc: 0.99, test_acc: 0.75\n",
      "Best acc! Saving model\n",
      "Epoch 122/200, train_loss: 0.037, train_acc: 1.0, test_acc: 0.75\n",
      "Best acc! Saving model\n",
      "Epoch 123/200, train_loss: 0.033, train_acc: 1.0, test_acc: 0.75\n",
      "Epoch 124/200, train_loss: 0.029, train_acc: 1.0, test_acc: 0.75\n",
      "Best acc! Saving model\n",
      "Epoch 125/200, train_loss: 0.026, train_acc: 1.0, test_acc: 0.75\n",
      "Epoch 126/200, train_loss: 0.025, train_acc: 1.0, test_acc: 0.75\n",
      "Best acc! Saving model\n",
      "Epoch 127/200, train_loss: 0.022, train_acc: 1.0, test_acc: 0.75\n",
      "Best acc! Saving model\n",
      "Epoch 128/200, train_loss: 0.02, train_acc: 1.0, test_acc: 0.76\n",
      "Best acc! Saving model\n",
      "Epoch 129/200, train_loss: 0.019, train_acc: 1.0, test_acc: 0.76\n",
      "Best acc! Saving model\n",
      "Epoch 130/200, train_loss: 0.019, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 131/200, train_loss: 0.018, train_acc: 1.0, test_acc: 0.76\n",
      "Best acc! Saving model\n",
      "Epoch 132/200, train_loss: 0.018, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 133/200, train_loss: 0.017, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 134/200, train_loss: 0.016, train_acc: 1.0, test_acc: 0.76\n",
      "Best acc! Saving model\n",
      "Epoch 135/200, train_loss: 0.017, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 136/200, train_loss: 0.016, train_acc: 1.0, test_acc: 0.76\n",
      "Best acc! Saving model\n",
      "Epoch 137/200, train_loss: 0.016, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 138/200, train_loss: 0.016, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 139/200, train_loss: 0.015, train_acc: 1.0, test_acc: 0.76\n",
      "Best acc! Saving model\n",
      "Epoch 140/200, train_loss: 0.015, train_acc: 1.0, test_acc: 0.76\n",
      "Best acc! Saving model\n",
      "Epoch 141/200, train_loss: 0.015, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 142/200, train_loss: 0.015, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 143/200, train_loss: 0.014, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 144/200, train_loss: 0.014, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 145/200, train_loss: 0.014, train_acc: 1.0, test_acc: 0.77\n",
      "Best acc! Saving model\n",
      "Epoch 146/200, train_loss: 0.014, train_acc: 1.0, test_acc: 0.77\n",
      "Best acc! Saving model\n",
      "Epoch 147/200, train_loss: 0.014, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 148/200, train_loss: 0.014, train_acc: 1.0, test_acc: 0.76\n",
      "Epoch 149/200, train_loss: 0.013, train_acc: 1.0, test_acc: 0.77\n",
      "Best acc! Saving model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e74ba783fd83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0a75ad8ad9bd>\u001b[0m in \u001b[0;36macc\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "best_model = None\n",
    "best_acc = -1.\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch < warmup_epochs:\n",
    "        warmup.step()\n",
    "    else:\n",
    "        scheduler.step(epoch)\n",
    "    \n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    model.eval()\n",
    "    _, _, train_acc = acc(model, train_loader)\n",
    "    _, _, test_acc = acc(model, test_loader)\n",
    "        \n",
    "    print(\"Epoch {}/{}, train_loss: {}, train_acc: {}, test_acc: {}\".format(\n",
    "        epoch, n_epochs,\n",
    "        round(epoch_loss / len(train_loader), 3),\n",
    "        round(train_acc, 2), round(test_acc, 2)\n",
    "    ))\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        print(\"Best acc! Saving model\")\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_acc = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
